# 分词在语义搜索中的作用

## 1. 核心概念

**分词（Tokenization）是将人类语言（文本）翻译成计算机能理解的数学语言（向量）的第一步，也是最关键的一步**。

如果没有分词，计算机无法理解“相近”是什么意思。

## 2. 为什么需要分词？（举例说明）

假设提示词库里有一条数据：
*   **库中数据**：“**React** **组件**的**性能**优化”

用户输入了查询：
*   **用户输入**：“如何提升 **React** **组件** **性能**”

### 如果没有分词：
计算机只能进行“字符串匹配”。
*   它会比较 `"React 组件的性能优化"` 和 `"如何提升 React 组件性能"`。
*   从字面上看，这两句话完全不一样，长度不同，字符也不同。计算机很难判断它们是相似的。

### 如果有分词（本项目使用的 jieba 分词）：
系统会先把这两句话切成一个个有意义的词（关键词）：

1.  **库中数据切分**：`["React", "组件", "性能", "优化"]` (去掉了“的”)
2.  **用户输入切分**：`["提升", "React", "组件", "性能"]` (去掉了“如何”)

## 3. 分词在算法中的具体作用

在 TF-IDF 算法流程中，分词起到了以下 3 个核心作用：

### A. 提取“特征” (Feature Extraction)
分词把一句完整的话，变成了几个核心的“特征点”。
在上面的例子中，系统发现两者都包含：`React`、`组件`、`性能`。
**3 个核心词重合！** 计算机立刻就能算出它们的相似度非常高（比如 0.8 分）。

### B. 过滤“噪音” (Stopword Removal)
我们在说话时有很多无意义的词，比如“的”、“了”、“是”、“么”。
*   分词器会配合“停用词表”（Stopwords）把这些干扰项去掉。
*   **作用**：让算法只关注 `React` 这种有实际含义的词，而不是去比较两个句子是不是都包含“的”。

### C. 解决中文特性 (Chinese Segmentation)
英文单词之间有空格（`Hello World`），天然就是分开的。
但中文是连在一起的（`你好世界`）。
*   如果不分词，计算机不知道 `你好世界` 是 `你好` + `世界`，还是 `你` + `好世` + `界`。
*   **jieba 分词** 的作用就是精准地把中文句子切开，让计算机知道这里面包含哪些词汇。

## 4. 总结流程图

您可以这样理解整个链路：

```mermaid
graph LR
    A[用户输入: "怎么写React组件"] -->|1. 分词 (关键步骤)| B(提取关键词)
    B --> C[关键词: "写", "React", "组件"]
    C -->|2. 向量化| D[数学向量: [0, 0.5, 0.8, ...]]
    D -->|3. 计算| E{与库中向量对比}
    E -->|4. 结果| F[找到相似: "React组件开发指南"]
```

## 结论

分词是**为了把“句子”变成“关键词列表”，从而让计算机能够统计词频，进而计算数学上的相似度。** 

不分词，就无法使用 TF-IDF 算法，也就无法实现“近似查找”功能。
